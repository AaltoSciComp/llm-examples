INFO 06-02 17:41:54 [__init__.py:239] Automatically detected platform cuda.
INFO 06-02 17:42:02 [config.py:600] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 06-02 17:42:02 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 06-02 17:42:03 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='Qwen/Qwen2.5-14B-Instruct-1M', speculative_config=None, tokenizer='Qwen/Qwen2.5-14B-Instruct-1M', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-14B-Instruct-1M, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-02 17:42:03 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x146c704ae2d0>
INFO 06-02 17:42:04 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-02 17:42:04 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 06-02 17:42:04 [gpu_model_runner.py:1258] Starting to load model Qwen/Qwen2.5-14B-Instruct-1M...
WARNING 06-02 17:42:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 06-02 17:42:04 [weight_utils.py:265] Using model weights format ['*.safetensors', '*.bin', '*.pt']
INFO 06-02 17:42:36 [loader.py:447] Loading weights took 32.14 seconds
INFO 06-02 17:42:36 [gpu_model_runner.py:1273] Model loading took 27.7982 GiB and 32.307211 seconds
INFO 06-02 17:42:44 [backends.py:416] Using cache directory: /home/tiany4/.cache/vllm/torch_compile_cache/d4bc4d4df1/rank_0_0 for vLLM's torch.compile
INFO 06-02 17:42:44 [backends.py:426] Dynamo bytecode transform time: 7.84 s
INFO 06-02 17:42:45 [backends.py:115] Directly load the compiled graph for shape None from the cache
INFO 06-02 17:42:51 [monitor.py:33] torch.compile takes 7.84 s in total
INFO 06-02 17:42:52 [kv_cache_utils.py:578] GPU KV cache size: 202,304 tokens
INFO 06-02 17:42:52 [kv_cache_utils.py:581] Maximum concurrency for 4,096 tokens per request: 49.39x
INFO 06-02 17:43:07 [gpu_model_runner.py:1608] Graph capturing finished in 15 secs, took 0.86 GiB
INFO 06-02 17:43:07 [core.py:162] init engine (profile, create kv cache, warmup model) took 30.77 seconds
Formatted text: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nTell me something about large language models.<|im_end|>\n<|im_start|>assistant\n'


===================OUTPUTS===================

Generated text: 'Large language models (LLMs) are advanced artificial intelligence systems designed to process and generate human-like text based on vast amounts of data they have been trained on. These models are characterized by their enormous size, often containing billions or even trillions of parameters, which allow them to capture complex patterns in natural language.\n\n### Key Features:\n1. **Scale**: LLMs like GPT-3, BERT, and others are trained on massive datasets, sometimes encompassing hundreds of billions of words from the internet, books, articles, and other sources.\n   \n2. **Generative Ability**: They can generate coherent and contextually relevant text, including essays, stories, code, emails, and more. This makes them versatile for various applications, such as content creation, translation, summarization, and even creative writing.\n\n3. **Contextual Understanding**: LLMs use deep learning techniques, particularly transformer architectures, to understand context across long sequences of text. This allows them to maintain coherence over multiple sentences or paragraphs.\n\n4. **Multitasking**: Many LLMs are capable of performing multiple tasks without requiring task-specific fine-tuning. For example, the same model can be used for translation, question answering, and summarization.\n\n5. **Ethical Considerations**: While powerful, LLMs also raise ethical concerns regarding bias, misinformation, privacy, and the potential misuse of generated content. Ensuring these models are fair, transparent, and safe is an ongoing challenge.\n\n6. **Applications**: Beyond general-purpose text generation, LLMs are being applied in fields like healthcare, finance, education, and customer service. They can assist in medical diagnosis, financial forecasting, personalized learning, and chatbots.\n\n7. **Continuous Learning**: Some LLMs can be updated with new data or fine-tuned for specific tasks, allowing them to improve over time.\n\nOverall, large language models represent a significant advancement in AI, offering unprecedented capabilities for natural language processing and interaction, but also presenting challenges that need to be carefully managed.'
Generated text: 'The capital of France is Paris.'
Generated text: "Photosynthesis is like a recipe that plants use to make their own food using sunlight! Here's how it works:\n\n1. **Sunlight**: Plants catch sunlight using a special pigment called chlorophyll, which is found in their leaves.\n2. **Water and Carbon Dioxide**: Plants take in water from the soil through their roots and carbon dioxide (CO₂) from the air.\n3. **Making Food**: Using the energy from sunlight, plants combine water and carbon dioxide to create glucose, which is a type of sugar they can use as food.\n4. **Oxygen**: As a byproduct, plants release oxygen into the air, which is important for us to breathe!\n\nSo, in short, photosynthesis helps plants grow by turning sunlight, water, and air into food and oxygen. It’s also why trees and plants are so important for life on Earth!"
Generated text: 'Beneath the sky, so vast and wide,  \nThe stars like diamonds softly glide.  \nThey whisper tales of ancient light,  \nThat traveled through the endless night.  \n\nEach spark a story, cold yet bright,  \nA distant dream in deepest black,  \nThey shimmer in the cosmic track,  \nAnd guide us home, though far from sight.'
