INFO 06-02 17:24:01 [__init__.py:239] Automatically detected platform cuda.
INFO 06-02 17:24:10 [config.py:600] This model supports multiple tasks: {'classify', 'embed', 'score', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 06-02 17:24:10 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 06-02 17:24:11 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='Qwen/Qwen2.5-14B-Instruct-1M', speculative_config=None, tokenizer='Qwen/Qwen2.5-14B-Instruct-1M', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-14B-Instruct-1M, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-02 17:24:12 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x149660392510>
INFO 06-02 17:24:13 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-02 17:24:13 [cuda.py:221] Using Flash Attention backend on V1 engine.
INFO 06-02 17:24:13 [gpu_model_runner.py:1258] Starting to load model Qwen/Qwen2.5-14B-Instruct-1M...
WARNING 06-02 17:24:13 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 06-02 17:24:13 [weight_utils.py:265] Using model weights format ['*.safetensors', '*.bin', '*.pt']
INFO 06-02 17:24:47 [loader.py:447] Loading weights took 34.07 seconds
INFO 06-02 17:24:48 [gpu_model_runner.py:1273] Model loading took 27.7982 GiB and 34.278267 seconds
INFO 06-02 17:24:57 [backends.py:416] Using cache directory: /home/tiany4/.cache/vllm/torch_compile_cache/d4bc4d4df1/rank_0_0 for vLLM's torch.compile
INFO 06-02 17:24:57 [backends.py:426] Dynamo bytecode transform time: 9.83 s
INFO 06-02 17:24:58 [backends.py:115] Directly load the compiled graph for shape None from the cache
INFO 06-02 17:25:06 [monitor.py:33] torch.compile takes 9.83 s in total
INFO 06-02 17:25:07 [kv_cache_utils.py:578] GPU KV cache size: 202,304 tokens
INFO 06-02 17:25:07 [kv_cache_utils.py:581] Maximum concurrency for 4,096 tokens per request: 49.39x
INFO 06-02 17:25:24 [gpu_model_runner.py:1608] Graph capturing finished in 18 secs, took 0.86 GiB
INFO 06-02 17:25:24 [core.py:162] init engine (profile, create kv cache, warmup model) took 36.64 seconds
Formatted text: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nTell me something about large language models.<|im_end|>\n<|im_start|>assistant\n'


===================OUTPUTS===================

Generated text: 'Large language models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text based on vast amounts of data. These models are characterized by their enormous size, often comprising billions or even trillions of parameters, which allow them to capture complex patterns in natural language.\n\n### Key Features:\n1. **Scale**: LLMs are trained on massive datasets that can include books, articles, websites, and other forms of written content. The larger the model, the more nuanced its understanding of language can be.\n   \n2. **Contextual Understanding**: These models use deep learning techniques, particularly transformer architectures, to process sequences of words and understand context. This allows them to generate coherent responses, summarize information, translate languages, and even engage in dialogue.\n\n3. **Generalization**: Due to their size and the diversity of the training data, LLMs can perform a wide range of tasks without being specifically fine-tuned for each one. They can generate creative writing, answer questions, provide explanations, and even assist in coding.\n\n4. **Limitations**: Despite their capabilities, LLMs have limitations. They may sometimes produce incorrect information, exhibit biases present in their training data, or generate harmful or inappropriate content. Careful monitoring and fine-tuning are necessary to mitigate these issues.\n\n5. **Applications**: LLMs are used in various fields, including customer service (chatbots), content creation, education, research, and even art. They are also being explored for applications in healthcare, law, and scientific discovery.\n\nIn summary, large language models represent a significant advancement in AI, offering powerful tools for natural language processing while also presenting challenges related to accuracy, bias, and ethical use.'
Generated text: 'The capital of France is Paris.'
Generated text: 'Photosynthesis is like a recipe that plants use to make their own food using sunlight! Hereâ€™s how it works:\n\n1. **Sunlight**: Plants capture sunlight using a special green substance in their leaves called chlorophyll.\n2. **Water and Carbon Dioxide**: They take water from the soil through their roots and carbon dioxide (a gas) from the air.\n3. **Making Food**: Using sunlight as energy, plants combine water and carbon dioxide to create glucose, which is a type of sugar they can use as food.\n4. **Oxygen**: As a byproduct, they release oxygen into the air, which is important for us to breathe.\n\nSo, photosynthesis helps plants grow and also gives us oxygen to breathe!'
Generated text: 'Beneath the vast, eternal sky,  \nThe stars like diamonds softly lie.  \nThey whisper secrets to the breeze,  \nOf distant lands and endless seas.  \n\nEach point of light, a dream unfurled,  \nA story told by ancient world.  \nThey guide the lost, they kiss the night,  \nAnd fill the dark with softest light.'
